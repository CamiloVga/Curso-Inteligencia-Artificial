{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNvjMD99ZJD0qDuLR3MlkdY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-Inteligencia-Artificial/blob/main/Clase_FineTuning_Classifier_PasoAPaso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning para un modelo de clasificación"
      ],
      "metadata": {
        "id": "5PNAejFygUJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación paso a paso para crear un modelo Fine-Tuning de un modelo Bert para la clasificación de texto en categorías. Se agrega la opción para crear datos sintéticos.\n",
        "\n",
        "**Creado por [Camilo Vega](https://www.linkedin.com/in/camilo-vega-169084b1/)**"
      ],
      "metadata": {
        "id": "N2HwLf5il9Py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generación datos sintéticos"
      ],
      "metadata": {
        "id": "xXJ0XpfHlmHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalar las librerías necesarias\n",
        "!pip install openai==0.28\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "# Paso 2: Configurar la clave de API de OpenAI\n",
        "# Reemplaza 'SU_CLAVE_DE_API' con tu clave de API de OpenAI\n",
        "openai.api_key = 'API KEY'\n",
        "\n",
        "# Paso 3: Definir variables para la generación de texto\n",
        "num_ejemplos = 100  # Número de ejemplos de texto a generar\n",
        "\n",
        "# Paso 4: Generar la base de datos con la API de OpenAI\n",
        "prompt = \"Escribe un trino sobre una reseña de un restaurante. El trino puede ser aleatoriamente bueno o malo sobre el restaurante\"\n",
        "base_datos = []\n",
        "for _ in range(num_ejemplos):\n",
        "    mensajes = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    respuesta = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Modelo de OpenAI utilizado\n",
        "        messages=mensajes,\n",
        "        max_tokens=50,  # Longitud máxima de la respuesta\n",
        "        temperature=1.5,  # Valor de temperatura para controlar la aleatoriedad\n",
        "        n=1,  # Número de respuestas a generar\n",
        "    )\n",
        "    texto = respuesta[\"choices\"][0][\"message\"]\n",
        "    contenido_mensaje = texto[\"content\"]\n",
        "    base_datos.append(contenido_mensaje)\n",
        "\n",
        "# Paso 5: Definir la instrucción de etiquetado\n",
        "instruccion_etiquetado = \"Clasifica el texto según las siguientes categorías de tono: Bueno, Malo. Solo responde con base a estas categorías, sin texto adicional ni puntos o espacios. Y solo se puede elegir una categoria\"\n",
        "\n",
        "# Paso 6: Crear la columna 'label' utilizando la API de OpenAI para el etiquetado\n",
        "etiquetas = []\n",
        "for texto in base_datos:\n",
        "    mensajes = [\n",
        "        {\"role\": \"system\", \"content\": instruccion_etiquetado},\n",
        "        {\"role\": \"user\", \"content\": texto}\n",
        "    ]\n",
        "    respuesta = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=mensajes,\n",
        "        max_tokens=150,\n",
        "        temperature=0.5,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    etiquetas.append(respuesta)\n",
        "\n",
        "# Paso 7: Combinar el texto y las etiquetas en un DataFrame\n",
        "df = pd.DataFrame({\"text\": base_datos, \"label\": etiquetas})\n",
        "\n",
        "# Paso 8: Guardar el DataFrame en un archivo CSV\n",
        "df.to_csv(\"train.csv\", index=False)\n",
        "\n",
        "# Paso 9: Mostrar el DataFrame resultante\n",
        "df\n"
      ],
      "metadata": {
        "id": "XNjFNm5pgfcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datos propios**\n",
        "\n",
        "También se puede usar un data set con datos propios. Es cuestión de seguir los siguientes pasos:\n",
        "\n",
        "1. Abrir Excel\n",
        "2. Crear un columna llamada \"text\" y otra \"label\"\n",
        "3. En la columna \"text\" poner los ejemplos de textos que se quieran clasificar\n",
        "4. En la columna \"label\" poner la respectiva etiqueta que aplique para cada texto.\n",
        "5. Guardar el archivo en formato .csv. Se recomienda guardarlo bajo el nombre train para que el siguiente código aplique a los datos propios.\n",
        "6. Subir los datos a Colba: ir al símbolo de la carpeta y en elegir el emogi de la hoja con la flecha dentro.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cG3tsyLHgrym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 10: Convertir las etiquetas a formato numérico\n",
        "# Mapear las clases a números\n",
        "etiquetas_numericas = {\"Bueno\": 0, \"Malo\": 1}\n",
        "etiquetas_numericas_convertidas = [etiquetas_numericas[etiqueta] for etiqueta in etiquetas]\n",
        "\n",
        "# Combinar el texto y las etiquetas numéricas en un nuevo DataFrame\n",
        "nuevo_df = pd.DataFrame({\"text\": base_datos, \"label\": etiquetas_numericas_convertidas})\n",
        "\n",
        "# Guardar el nuevo DataFrame en un archivo CSV\n",
        "nuevo_df.to_csv(\"train_numerico.csv\", index=False)\n",
        "\n",
        "# Mostrar el nuevo DataFrame resultante\n",
        "nuevo_df"
      ],
      "metadata": {
        "id": "ZjQX1MGygqSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "GWt5-Euzkp4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 11: Instalar los paquetes necesarios para el entrenamiento del modelo\n",
        "!pip install transformers datasets  # Instalar paquetes transformers y datasets\n",
        "!pip install transformers[torch]  # Instalar transformers con soporte para PyTorch\n",
        "!pip install accelerate  # Instalar accelerate\n",
        "!pip install evaluate  # Instalar evaluate\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset  # Importar Dataset de datasets\n",
        "from transformers import AutoTokenizer  # Importar AutoTokenizer de transformers\n",
        "from transformers import AutoModelForSequenceClassification  # Importar AutoModelForSequenceClassification de transformers\n",
        "from transformers import TrainingArguments, Trainer  # Importar TrainingArguments y Trainer de transformers\n",
        "import numpy as np  # Importar numpy\n",
        "import evaluate  # Importar evaluate\n",
        "\n",
        "# Paso 12: Cargar el conjunto de datos desde el archivo CSV\n",
        "dataset = pd.read_csv(\"train_numerico.csv\")\n",
        "\n",
        "# Paso 13: Dividir el conjunto de datos en conjuntos de entrenamiento y evaluación\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Asumiendo que tienes columnas llamadas \"text\" y \"label\"\n",
        "train_texts, eval_texts, train_labels, eval_labels = train_test_split(dataset[\"text\"], dataset[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear diccionarios para los conjuntos de entrenamiento y evaluación\n",
        "train_dataset = {\"text\": train_texts, \"label\": train_labels}\n",
        "eval_dataset = {\"text\": eval_texts, \"label\": eval_labels}\n",
        "\n",
        "# Convertir diccionarios a objetos Dataset\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "eval_dataset = Dataset.from_dict(eval_dataset)\n",
        "\n",
        "# Paso 14: Cargar el tokenizador BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")  # Cargar tokenizador BERT\n",
        "\n",
        "# Paso 15: Definir la función para tokenizar\n",
        "def tokenize_function(examples):  # Definir función para tokenizar\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "# Paso 16: Aplicar tokenización al conjunto de datos\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)  # Aplicar tokenización al conjunto de entrenamiento\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)  # Aplicar tokenización al conjunto de evaluación\n",
        "\n",
        "# Paso 17: Agregar las etiquetas al conjunto de datos tokenizado\n",
        "tokenized_train_dataset = tokenized_train_dataset.add_column(\"labels\", train_labels)\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.add_column(\"labels\", eval_labels)\n",
        "\n",
        "# Paso 18: Cargar el modelo BERT para clasificación de secuencias\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=len(dataset[\"label\"].unique()))  # Cargar modelo BERT para clasificación de secuencias\n",
        "\n",
        "# Paso 19: Definir hiperparámetros de entrenamiento\n",
        "training_args = TrainingArguments(output_dir=\"restaurants-reviews\", evaluation_strategy=\"epoch\")  # Definir hiperparámetros de entrenamiento con evaluación por época\n",
        "\n",
        "# Paso 20: Cargar la métrica de precisión\n",
        "metric = evaluate.load(\"accuracy\")  # Cargar métrica de precisión\n",
        "\n",
        "# Paso 21: Definir la función para calcular métricas\n",
        "def compute_metrics(eval_pred):  # Definir función para calcular métricas\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "Hdbvv972kwPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 22: Crear el objeto Trainer (continuación)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")  # Crear objeto Trainer\n",
        "\n",
        "# Paso 23: Entrenar el modelo\n",
        "trainer.train()  # Entrenar el modelo"
      ],
      "metadata": {
        "id": "C0jxQT-Lk3EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probar el modelo"
      ],
      "metadata": {
        "id": "dQCq4j7Ek5hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 24: Importar el pipeline de transformers\n",
        "from transformers import pipeline  # Importar pipeline de transformers\n",
        "\n",
        "# Paso 25: Definir un prompt de ejemplo\n",
        "prompt = \"food is very very good, and perfect service\"\n",
        "\n",
        "# Paso 26: Crear el pipeline de clasificación de texto\n",
        "pipe = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)  # Crear pipeline de clasificación de texto\n",
        "\n",
        "# Paso 27: Realizar inferencia con el texto de ejemplo\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST] \\n\\n\")  # Realizar inferencia con texto de ejemplo\n",
        "\n",
        "# Paso 28: Imprimir el resultado\n",
        "print(result)  # Imprimir resultado\n",
        "\n"
      ],
      "metadata": {
        "id": "PjYZ2vBMlBAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 29: Mejorar la respuesta del modelo\n",
        "\n",
        "# Diccionario para convertir etiquetas numéricas a etiquetas originales\n",
        "etiquetas_originales = {\"LABEL_0\": \"Bueno\", \"LABEL_1\": \"Malo\"}\n",
        "\n",
        "# Paso 30: Definir la función para convertir predicciones\n",
        "def convertir_predicciones(predicciones):\n",
        "    etiquetas_predichas = []\n",
        "    for prediccion in predicciones:\n",
        "        etiqueta_predicha = None\n",
        "        for clave, valor in etiquetas_originales.items():\n",
        "            if clave == prediccion:\n",
        "                etiqueta_predicha = valor\n",
        "                break\n",
        "        if etiqueta_predicha is None:\n",
        "            etiqueta_predicha = prediccion\n",
        "        etiquetas_predichas.append(etiqueta_predicha)\n",
        "    return etiquetas_predichas"
      ],
      "metadata": {
        "id": "BOqPRMgvlJuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 31: Ejemplo de inferencia y conversión de predicciones\n",
        "prompt = \"food is very very nice\"\n",
        "pipe = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "result = pipe(prompt)[0]\n",
        "prediccion_numerica = result['label']\n",
        "etiqueta_predicha = convertir_predicciones([prediccion_numerica])[0]\n",
        "print(f\"Predicción: {etiqueta_predicha}\")"
      ],
      "metadata": {
        "id": "9RnkIhJqlLnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XmZDX6GRlO1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subir el modelo a HuggingFace"
      ],
      "metadata": {
        "id": "XzaezDLWlUIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 32: Importar la función notebook_login de huggingface_hub\n",
        "from huggingface_hub import notebook_login  # Importar notebook_login de huggingface_hub\n",
        "\n",
        "# Paso 33: Iniciar sesión en Hugging Face Hub\n",
        "notebook_login()  # Iniciar sesión en Hugging Face Hub\n",
        "\n"
      ],
      "metadata": {
        "id": "0H7h7evplZcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 34: Publicar el modelo en Hugging Face Hub\n",
        "trainer.push_to_hub()  # Publicar modelo en Hugging Face Hub"
      ],
      "metadata": {
        "id": "9i3-sOgelb9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}